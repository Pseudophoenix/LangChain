{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abef1fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community langchain-openai langchainhub chromadb langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a766ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a417300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langchain_api_key\n",
    "import os\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289f0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "# This is the retriever we will use in RAG\n",
    "# This is mocked out, but it could be anything we want\n",
    "def retriever(query: str):\n",
    "    results = [\"Harrison worked at Kensho\"]\n",
    "    return results\n",
    "\n",
    "# This is the end-to-end RAG chain.\n",
    "# It does a retrieval step then calls OpenAI\n",
    "def rag(question):\n",
    "    docs = retriever(question)\n",
    "    system_message = \"\"\"Answer the users question using only the provided information below:\n",
    "    \n",
    "    {docs}\"\"\".format(docs=\"\\n\".join(docs))\n",
    "    \n",
    "    return openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffe275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "#### INDEXING ####\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "#### RETRIEVAL and GENERATION ####\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc03604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ea0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd0b757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e71f543d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# document embedding representation in vector space\n",
    "# get nearby documents having similar semantics\n",
    "\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d6a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "len(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db618a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e552d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4508c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39554dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c54c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3.1\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22636eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Changed model to Gemma 3:4B\n",
    "model = OllamaLLM(model=\"gemma:3b\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5164000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A nice opportunity to dive into a fascinating topic!\\n\\nAlright, let's break it down step by step.\\n\\nLangChain refers to a type of AI language model that combines the strengths of different pre-trained language models (PTLMs) and applies them in a specific way to generate human-like text. Here's a simplified overview:\\n\\n1. **Multi-PTLM approach**: LangChain uses multiple pre-trained language models, each with its unique capabilities and strengths. For example, one might be good at generating coherent text, while another excels at producing creative writing.\\n2. **Hybrid architecture**: The model combines the outputs from these individual PTLMs to create a hybrid architecture that leverages the best of each. This allows LangChain to capitalize on the diversity of the input models and generate more robust and accurate text.\\n3. **Iterative refinement**: LangChain uses an iterative process to refine its output, allowing it to adapt and improve over time. This might involve re-running the model multiple times with small adjustments or incorporating feedback from users.\\n\\nBy combining the strengths of different PTLMs and refining the output through iteration, LangChain can produce high-quality text that is often indistinguishable from human-written content. Its applications are vast, including language translation, content generation, and even creative writing.\\n\\nWould you like me to elaborate on any specific aspect of LangChain?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"What is LangChain?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6b40b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl http://localhost:11434/api/generate -d '{\"model\": \"llama2\",\"prompt\":\"Why is the sky blue?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a734ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"New York refers to the state of New York in the northeastern United States. It's a popular tourist destination known for its iconic city, New York City (also called the Big Apple).\\n\\nNow, let's talk about the distance from Delhi, India.\\n\\nThe time difference between Delhi and New York is approximately 9 hours and 30 minutes ahead of New York during standard time (UTC/GMT +5:30). During daylight saving time in the US (UTC/GMT -4), it would be 8 hours and 30 minutes ahead of New York.\\n\\nNow, about the distance:\\n\\n* Delhi, India to New York City, USA:\\n\\t+ Distance by air: approximately 7,760 miles (12,460 kilometers)\\n\\t+ Flight duration: around 14-16 hours with a layover or non-stop flights\\n* Delhi, India to Albany, New York, USA (the state capital):\\n\\t+ Distance by air: approximately 7,640 miles (12,320 kilometers)\\n\\t+ Flight duration: around 13-15 hours with a layover or non-stop flights\\n\\nKeep in mind that these distances and flight durations are approximate and may vary depending on the specific routes taken.\\n\\nHope this helps!\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3\")\n",
    "llm.invoke(\"Where is New York and how far is it from delhi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from fastapi import FastAPI\n",
    "from langchain.llms import Ollama\n",
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langserve import add_routes\n",
    "import uvicorn\n",
    "\n",
    "llama2 = Ollama(model=\"mistral\")\n",
    "template = PromptTemplate.from_template(\"Tell me a joke about {topic}.\")\n",
    "chain = template | llama2 | CommaSeparatedListOutputParser()\n",
    "\n",
    "app = FastAPI(title=\"LangChain\", version=\"1.0\", description=\"The first server ever!\")\n",
    "add_routes(app, chain, path=\"/chain\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
